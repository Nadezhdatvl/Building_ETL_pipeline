# Building_ETL_pipeline
Karpov courses

# Проект 1: Построение ETL-Пайплайна (DAG_ETL_TVL)
#### Описание проекта:
В этом проекте я буду решать задачу на ETL схему и как это можно сделать при помощи библиотеки Airflow.
В целом Airflow — это удобный инструмент для решения ETL-задач.
Ожидается, что на выходе будет DAG в Airflow, который будет считаться каждый день за вчера. Т.е. каждый день таблица должна дополняться новыми данными.

#### Этапы работы:
* В feed_actions для каждого юзера считаю число просмотров и лайков контента.
* В message_actions для каждого юзера считаю, сколько он получает и отсылает сообщений, скольким людям он пишет, сколько людей пишут ему.
* Далее объединяю две таблицы в одну.
* Для этой таблицы считаю все эти метрики в разрезе по полу, возрасту и ос. Делаю три разных таска на каждый срез.
* И финальные данные со всеми метриками записываю в отдельную таблицу в ClickHouse.

#### Стек:
ClickHouse, SQL Lab, JUPYTERHUB, Redash, Airflow, Python (pandas, pandahouse, datetime, timedelta, numpy).
Для создания DAG-а и его «содержания»: airflow, DAG,task,  airflow.operators.python_operator, PythonOperator, datetime, airflow.decorators


# Проект 2: Автоматизация отчетности (DAG_report_TVL, DAG_report_mess_TVL)
#### Описание проекта:
Автоматизировать базовую отчетность приложения. Наладить автоматическую отправку аналитической сводки в телеграмм каждое утро!

#### Этапы работы:
* Создаю своего телеграмм-бота с помощью @botfather. Чтобы получить chat_id, воспользовалась ссылкой https://api.telegram.org/bot<токен_вашего_бота>/getUpdates  или методом bot.getUpdates()
* Напиcала скрипт для сборки отчета по ленте новостей.
  Отчет состоит из двух частей:
  1.Текст с информацией о значениях ключевых метрик за предыдущий день
  2.График с значениями метрик за предыдущие 7 дней
* Отобразила в отчете следующие ключевые метрики: DAU, Просмотры, Лайки, CTR
* Автоматизировала отправку отчета с помощью Airflow. Код для сборки отчета разместила в GitLab
* Отчет приходит ежедневно в 11:00 в свой чат.

#### Стек:
ClickHouse, SQL Lab, JUPYTERHUB, Redash, Airflow, Python (pandas, pandahouse, datetime, timedelta, numpy, telegram, matplotlib.pyplot, seaborn, io). Для создания DAG-а и его «содержания»: airflow, DAG,task,  airflow.operators.python_operator, PythonOperator, datetime, airflow.decorators, get_current_context


# Проект 3: Поиск аномалий. Система алертов. (DAG_alert_TVL)
#### Описане проекта:
Собрать единый отчет по работе всего приложения. В отчете должна быть информация и по ленте новостей, и по сервису отправки сообщений. 
Отчет должен быть не просто набором графиков или текста, а помогать отвечать бизнесу на вопросы о работе всего приложения совокупно.
Система с периодичность каждые 15 минут проверяет ключевые метрики, такие как активные пользователи в ленте / мессенджере, просмотры, лайки, CTR, количество отправленных сообщений.

#### Этапы работы:
* Изучила поведение метрик и подобрала наиболее подходящий алгоритм поиска аномалий по ключевым метрикам. Использовала межквартильный размах.
* В случае обнаружения аномального значения, в чат отправляется алерт - сообщение со следующей информацией: метрика, ее значение, величина отклонения, график и ссылки на дашборд/чарт в BI системе.
* DAG запускается с интервалом 15 минут.

#### Стек:
ClickHouse, SQL Lab, JUPYTERHUB, Redash, Airflow, Python (pandas, pandahouse, datetime, timedelta, numpy, telegram, matplotlib.pyplot, seaborn, io). Для создания DAG-а и его «содержания»: airflow, DAG,task,  airflow.operators.python_operator, PythonOperator, datetime, airflow.decorators, get_current_context
